
@phdthesis{mariette_apprentissage_nodate,
	title = {Apprentissage statistique pour l'intégration de données omiques},
	author = {Mariette, Jerôme},
	file = {Mariette - Apprentissage statistique pour l'intégration de do.pdf:/Users/kylliann/Zotero/storage/BFDEMCWV/Mariette - Apprentissage statistique pour l'intégration de do.pdf:application/pdf}
}

@article{kang_unified_nodate,
	title = {Unified {Spectral} {Clustering} with {Optimal} {Graph}},
	abstract = {Spectral clustering has found extensive use in many areas. Most traditional spectral clustering algorithms work in three separate steps: similarity graph construction; continuous labels learning; discretizing the learned labels by k-means clustering. Such common practice has two potential ﬂaws, which may lead to severe information loss and performance degradation. First, predeﬁned similarity graph might not be optimal for subsequent clustering. It is well-accepted that similarity graph highly affects the clustering results. To this end, we propose to automatically learn similarity information from data and simultaneously consider the constraint that the similarity matrix has exact c connected components if there are c clusters. Second, the discrete solution may deviate from the spectral solution since k-means method is well-known as sensitive to the initialization of cluster centers. In this work, we transform the candidate solution into a new one that better approximates the discrete one. Finally, those three subtasks are integrated into a uniﬁed framework, with each subtask iteratively boosted by using the results of the others towards an overall optimal solution. It is known that the performance of a kernel method is largely determined by the choice of kernels. To tackle this practical problem of how to select the most suitable kernel for a particular data set, we further extend our model to incorporate multiple kernel learning ability. Extensive experiments demonstrate the superiority of our proposed method as compared to existing clustering approaches.},
	language = {en},
	author = {Kang, Zhao and Peng, Chong and Cheng, Qiang and Xu, Zenglin},
	pages = {8},
	file = {Kang et al. - Unified Spectral Clustering with Optimal Graph.pdf:/Users/kylliann/Zotero/storage/BGSVH247/Kang et al. - Unified Spectral Clustering with Optimal Graph.pdf:application/pdf}
}

@article{ren_multiple_2020,
	title = {Multiple kernel subspace clustering with local structural graph and low-rank consensus kernel learning},
	volume = {188},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705119304393},
	doi = {10.1016/j.knosys.2019.105040},
	abstract = {Multiple kernel learning (MKL) methods are generally believed to perform better than single kernel learning (SKL) methods in handling nonlinear subspace clustering problem, largely thanks to MKL avoids selecting and tuning a pre-defined kernel. However, previous MKL methods mainly focused on how to define a kernel weighting strategy, but ignored the structural characteristics of the input data in both the original space and the kernel space. In this paper, we first propose a novel graphbased MKL method for subspace clustering, namely, Local Structural Graph and Low-Rank Consensus Multiple Kernel Learning (LLMKL). It jointly learns an optimal affinity graph and a suitable consensus kernel for clustering purpose by elegantly integrating the MKL technology, the global structure in the kernel space, the local structure in the original space, and the Hilbert space self-expressiveness property in a unified optimization model. In particular, to capture the data global structure, we employ a substitute of the desired consensus kernel, and then introduce a low-rank constraint on the substitute to encourage that the structure of linear subspaces is present in the feature space. Moreover, the data local structure is explored by building a complete graph, where each sample is treated as a node, and an edge codes the pairwise affinity between two samples. By such, the consensus kernel learning and the affinity graph learning can promote each other such that the data in resulting Hilbert space are both self-expressive and low-rank. Experiments on both image and text clustering well demonstrate that LLMKL outperforms the state-of-the-art methods.},
	language = {en},
	urldate = {2021-05-21},
	journal = {Knowledge-Based Systems},
	author = {Ren, Zhenwen and Li, Haoran and Yang, Chao and Sun, Quansen},
	month = jan,
	year = {2020},
	pages = {105040},
	file = {Ren et al. - 2020 - Multiple kernel subspace clustering with local str.pdf:/Users/kylliann/Zotero/storage/G2EGTXZI/Ren et al. - 2020 - Multiple kernel subspace clustering with local str.pdf:application/pdf}
}

@article{von_luxburg_tutorial_2007,
	title = {A {Tutorial} on {Spectral} {Clustering}},
	url = {http://arxiv.org/abs/0711.0189},
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved eﬃciently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the ﬁrst glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe diﬀerent graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several diﬀerent approaches. Advantages and disadvantages of the diﬀerent spectral clustering algorithms are discussed.},
	language = {en},
	urldate = {2021-05-21},
	journal = {arXiv:0711.0189 [cs]},
	author = {von Luxburg, Ulrike},
	month = nov,
	year = {2007},
	note = {arXiv: 0711.0189},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
	file = {von Luxburg - 2007 - A Tutorial on Spectral Clustering.pdf:/Users/kylliann/Zotero/storage/KY795JNN/von Luxburg - 2007 - A Tutorial on Spectral Clustering.pdf:application/pdf}
}

@article{wen_feasible_2013,
	title = {A feasible method for optimization with orthogonality constraints},
	volume = {142},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/s10107-012-0584-1},
	doi = {10.1007/s10107-012-0584-1},
	language = {en},
	number = {1-2},
	urldate = {2021-05-21},
	journal = {Mathematical Programming},
	author = {Wen, Zaiwen and Yin, Wotao},
	month = dec,
	year = {2013},
	pages = {397--434},
	file = {Version soumise:/Users/kylliann/Zotero/storage/AWPWATCP/Wen et Yin - 2013 - A feasible method for optimization with orthogonal.pdf:application/pdf}
}

@article{schonemann_generalized_1966,
	title = {A generalized solution of the orthogonal procrustes problem},
	volume = {31},
	issn = {0033-3123, 1860-0980},
	url = {http://link.springer.com/10.1007/BF02289451},
	doi = {10.1007/BF02289451},
	language = {en},
	number = {1},
	urldate = {2021-05-21},
	journal = {Psychometrika},
	author = {Schönemann, Peter H.},
	month = mar,
	year = {1966},
	pages = {1--10}
}
