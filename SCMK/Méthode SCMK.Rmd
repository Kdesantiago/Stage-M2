---
title: "Méthode SCMK"
header-includes:
   - \usepackage{algorithm}
   - \usepackage{algorithmic}
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# 1. Idée de la méthode :

Le Spectral Clustering with Multiple Kernels (SCMK) est une méthode de clustering voulant utilisé à la fois le Spectral Clustering (SC) et la création de matrice d'affinité afin d'améliorer les performances de clustering.

La différence principale entre le SCMK et le Spectral Clustering est que le SC est un algorithme en deux étapes alors que le SCMK, non.
En effet, lorsque l'on utilise le SC, on commence par construire une représentation des données qui cherche à correspondre aux affinités réelles, puis on applique la méthode du spectral clustering pour grouper les individus.

Pour le SCMK, ces étapes sont faites de manière simultanée, on va chercher à apprendre une matrice d'affinité reflétant la relation de nos individus ainsi que la création du cluster "optimal". Ces étapes dépendent de paramètres qui sont mis à jour au fil de l'algorithme, et intéragissent entre eux, permettant de profitre pleinement de l'information générée par chacune des étapes.

# 2. Fonction d'objectif

La fonction d'objectif que nous avons avec le SCMK est :

$$
\min_{Z,F,P,Q,w} \underbrace{\text{Tr}(K_w - 2K_wZ +Z^TK_wZ) + \alpha||Z||_1}_{\text{Self-expressiveness}} + \underbrace{\beta\ \text{Tr}(P^TLP)}_{\text{SC - seuillage doux}} + \underbrace{\gamma \ || F - PQ||^2_F}_{\text{SC - seuillage dur}}
$$
s.c. $Z \geq 0$ ($\forall (i,j) \ Z_{ij} \geq 0$),  $\text{diag}(Z)=0$, $P^TP = I$, $Q^TQ=I$, $F \in \text{Idx}$, $K_w = \sum_{i=1}^r w_iK^i$, $\sum_{i=1}^r \sqrt{w_i}=1$, $w_i\geq 0$

où Tr est la fonction Trace, et $\alpha$, $\beta$, $\gamma$ sont les paramètres de penalités.

Notre fonction d'objectif est répartie en 3 termes + une condition de noyau consensus :

- Self-expressiveness : $\min_Z{\text{Tr}(K - 2KZ +Z^TKZ) + \alpha||Z||_1}$ s.c. $Z \geq 0$,  $\text{diag}(Z)=0$.

- Spectral Clustering - seuillage doux : $\min_{P} \gamma\ \text{Tr}(P^TLP) ,\ \text{s.c.} \ P^TP = I$

- Spectral Clustering - seuillage dur : $\min_{F,Q}\gamma || F - PQ||^2_F, \ \text{s.c.} \ Q^TQ = I \text{ et } F \in \text{Idx}$

- Noyau consensus : $K_w = \sum_{i=1}^r w_iK^i$, $\sum_{i=1}^r \sqrt{w_i}=1$, $w_i\geq 0$

## 2.1. Self-expressiveness

L'idée derrière le `self_expressiveness` est qu'une donnée peut être reconstruit comme combinaison linéaire des autres points, et on ajoute un terme de pénalité pour imposer une parcimonie. 

A l'origine, le problème est formulé de la façon suivante :

$$
\min_Z ||X-XZ||^2_F + \alpha ||Z||_1,\ \text{s.c.} \  Z \geq 0,\text{diag}(Z)=0
$$

$Z$ est une matrice d'affinité. Elle permet de représenter le lien entre les données, et met un poids plus important pour des individus proches. Le terme de pénalité permet d'avoir une représentation sparse, donnant ainsi une représentation plus claire entre les données.

Le souci de cette formulation est qu'on suppose que tous les points se trouvent dans une union de sous-espaces indépendants/disjoints, et que les données sont non bruitées. Dans le cas où la structure des données ne répond pas à ces cadres, la représentation sera parasitée, et donc moins précise.

Pour cela, on peut généraliser la formule précédente par la suivante :

$$
\min_Z ||\phi(X)-\phi(X)Z||^2_F + \alpha ||Z||_1 ,\ \text{s.c.} \  Z \geq 0,\text{diag}(Z)=0
$$
Puis, avec un peu de travail on arrive à :
$$
\min_Z ||\phi(X)-\phi(X)Z||^2_F + \alpha ||Z||_1 \Leftrightarrow \ ... \ \Leftrightarrow \ \min_Z \text{Tr}(K - 2KZ +Z^TKZ) + \alpha||Z||_1
$$
s.c. $Z \geq 0$,  $\text{diag}(Z)=0$ et où la matrice $K = \phi(X)^T.\phi(X)$ est un kernel que l'on a défini en amont.

On arrive donc à la quantité que l'on cherche à minimiser, en $Z$, suivante :

$$
\min_Z{\text{Tr}(K - 2KZ +Z^TKZ) + \alpha||Z||_1}
$$
s.c. $Z \geq 0$,  $\text{diag}(Z)=0$.

Avec cette écriture, le modèle trouve les relations linéaires entre les données dans le nouvel espace obtenu grâce au kernel, et par conséquent, des relations non linéaires dans la représentation originale.

## 2.2. Spectral Clustering - seuillage doux

Dans notre cas, nous supposons avoir c clusters. L'objectif du spectral clustering est de trouver F tel que :
$$
\min_{F \in \text{Idx}} \text{Tr}(F^TLF)
$$
où $F \in \{ 0,1 \}^{n \times c}$ avec $F_{i,j} = 1$ si l'individu $i$ est dans le cluster $j$, et $0$ sinon ; $L$ est le laplacien.

On nommera F la matrice de clustering dur, car elle indique directement dans quel groupe ce situe les individus.

Ce problème étant très complexe par la contrainte de discrétisation des valeurs de F, on peut relaxer celle-ci pour avoir la formulation plus habituelle du spectral clustering :

$$
\min_{P} \text{Tr}(P^TLP) ,\ \text{s.c.} \ P^TP = I
$$
où $I$ représente la matrice identitée, et $P \in \mathbb{R}^{n \times c}$.

On nommera $P$ la matrice de clusterig doux.

Par ailleurs, pour revenir à la matrice $F$, on utilise une méthode de clustering classique comme les K-means sur $P$.

C'est par les points précédents que nous avons le terme suivant dans la fonction d'objectif du SCMK :

$$
\min_P{\beta \ \text{Tr}(P^TLP)}, \ \text{s.c.} \ P^TP = I
$$


## 2.3. Spectral Clustering - seuillage dur

Cette partie concerne le lien entre les matrice $P$ et $F$, par l'intermédiaire d'une matrice de rotation $Q$.
Si $P$ est une solution du Spectral clustering, par invariance, $PQ$ est aussi solution. 
L'idée est d'avoir $P$ et $Q$ de telle manière que $PQ$ soit le plus proche possible de la "vraie" matrice de clustering, la matrice de clustering dur.

$$
\min_{F,Q}\gamma || F - PQ||^2_F, \ \text{s.c.} \ Q^TQ = I \text{ et } F \in \text{Idx} 
$$

## 2.4 Noyau consensus

La matrice d'affinité dépend grandement du Kernel utilisé, dont dépend d'un choix a priori que l'on fait lors de la sélection de l'espace dans lequel on projette nos données. Afin de rendre ce choix moins important au sens de l'impact sur les performances de la méthode, on peut non plus se limiter à un seul kernel mais à une combinaison convexe de ceux-ci ; le but étant d'arriver à extraire un maximum d'information permettant de mieux regrouper les données à partir des différents kernels.

Dans cette méthode, c'est de cette façon que les différents kernels sont assemblés, on pondère l'information de chaque kernel en fonction de sa pertinence, de sa contribution, pour le clustering. C'est pour cela que dans nos autres termes nous trouvons $K_w$ le noyau consensus, et qu'il y a les conditions suivantes dans les contraintes :

$$
K_w = \sum_{i=1}^r w_iK^i, \sum_{i=1}^r \sqrt{w_i}=1 \text{ et } w_i\geq 0
$$

Par ailleurs, ici nous avons la condition que $\sum_{i=1}^r \sqrt{w_i}=1$ car si on pose $\phi_w(x) = [\sqrt{w_1}\phi_1(x),...,\sqrt{w_r}\phi_r(x)]$, alors :

$$
K_w(x,y) = <\phi_w(x),\phi_w(y)> = \sum_{i=1}^r w_iK^i(x,y)
$$

# 3. Algorithme et optimisation des paramètres

Afin d'optimiser la fonction d'objectif, on va utiliser la méthode d'optimisation Alternating Direction Method of Multipliers (ADMM) ou Algorithme des directions alternées en français. Cette méthode utilise le lagrangien augmentée et à la particularité de permettre, à chaque étape d'optimisation, de fixer tous les paramètres et les optimiser un à un.

## 3.1. Lagrangien augmenté

Le problème de minimisation peut se réécrire sous la forme suivante :

\begin{equation*}
\begin{gathered}
\min_{Z,F,P,Q,w} \text{Tr}(K_w - 2K_wZ +Z^TK_wZ) + \alpha||S||_1 + \beta\ \text{Tr}(P^TLP) + \gamma \ || F - PQ||^2_F\\ 
s.c.\  Z \geq 0,  \text{diag}(Z)=0, P^TP = I, Q^TQ=I, F \in \text{Idx}, K_w = \sum_{i=1}^r w_iK^i, \sum_{i=1}^r \sqrt{w_i}=1, S=Z.
\end{gathered}
\end{equation*}

Dans la norme 1 nous avons fait un changement de variable, en mettant la matrice S à la place de la matrice Z, puis nous avons imposé une contrainte d'égalité entre les termes. Le lagrangien augmenté qui en découle est de la forme :
$$
{L}(S,Z,Y) = \text{Tr}(K_w - 2K_wZ +Z^TK_wZ) + \alpha||Z||_1 + \beta\ \text{Tr}(P^TLP) + \gamma \ || F - PQ||^2_F +\frac{\mu}{2}||S-Z+\frac{Y}{\mu}||^2_F
$$

## 3.2. Mise à jour des paramètres

### 3.2.1. S

Tout d'abord pour la matrice $S$, on souhaite la trouver telle qu'elle correspond à :
$$
\underset{S}{\mathrm{argmin}} \ \alpha||S||_1 + \frac{\mu}{2}||S-Z+\frac{Y}{\mu}||^2_F
$$

On constate que l'on peut réécrire le problème sous la forme suivante :

\begin{equation*}
\begin{split}
\underset{S_{ij}}{\mathrm{argmin}} \ \alpha||S_{ij}||_1 + \frac{\mu}{2}||S_{ij}-Z_{ij}+\frac{Y_{ij}}{\mu}||^2_F
&= \underset{S_{ij}}{\mathrm{argmin}} \ \alpha|S_{ij}| + \frac{\mu}{2}(S_{ij}-Z_{ij}+\frac{Y_{ij}}{\mu})^2\\
&=\underset{S_{ij}}{\mathrm{argmin}} \ \alpha|S_{ij}| + \frac{\mu}{2}(S_{ij}-H_{ij})^2, \forall (i,j) \in \{1,...,n\}^2
\end{split}
\end{equation*}
avec $H_{ij} = Z_{ij}-\frac{Y_{ij}}{\mu}$ 
On peut donc "dériver" cette quantité par rapport à $S_{ij}$, et annuler celle-ci. On distingue 3 cas :

- $S_{ij} > 0$ : la dérivée de $|S_{ij}| = 1$, donc on a : $\alpha + \mu(S_{ij}-H_{ij}) = 0 \Leftrightarrow S_{ij} = H_{ij} - \frac{\alpha}{\mu}$ pour $H_{ij} > \frac{\alpha}{\mu}$

- $S_{ij} < 0$ : la dérivée de $|S_{ij}| = -1$, donc on a : $-\alpha + \mu(S_{ij}-H_{ij}) = 0 \Leftrightarrow S_{ij} = H_{ij} + \frac{\alpha}{\mu}$ pour $H_{ij} < - \frac{\alpha}{\mu}$

- $S_{ij} = 0$ : Les sous-gradients de la norme 1 varient entre ]-1,1[ mais la valeur de $S_{ij}$ reste à 0.

Donc, on met à jour S par l'équation :

\begin{equation}
S_{ij} = \max(|H_{ij}|-\frac{\alpha}{\mu})\ . \ \text{sign}(H_{ij}), \ \forall(i,j)
\end{equation}

### 3.2.2. Z

On commence par poser des notations :

- $E = S + \frac{Y}{\mu}$ 

- $D$ tel que $D_{i,j} = ||P_{i,:} - P_{j,:}||^2_2$ 

- $F(Z) = \text{Tr}(K_w - 2K_wZ +Z^TK_wZ) + \beta \ \text{Tr}(P^TLP) + \frac{\mu}{2}||S-Z+\frac{Y}{\mu}||^2_F$

Par ailleurs, on remarquera que  $\sum_{ij}\frac{1}{2}||P_{i,:} - P_{j,:}||^2_2 s_{ij} = \text{Tr}(P^TLP)$.

On cherche donc : 

\begin{equation*}
\begin{split}
\underset{Z}{\mathrm{argmin}} F(Z)&= \underset{Z}{\mathrm{argmin}} \text{Tr}(K_w - 2K_wZ +Z^TK_wZ) + \beta \ \text{Tr}(P^TLP) + \frac{\mu}{2}||E-Z||^2_F\\
&=\underset{Z}{\mathrm{argmin}}\text{Tr}( -2K_wZ +Z^TK_wZ) + \frac{\beta}{2} \ \sum_{ij}||P_{i,:} - P_{j,:}||^2_2 Z_{ij} + \frac{\mu}{2}||E-Z||^2_F\\
&=\underset{Z}{\mathrm{argmin}} \text{Tr}(- 2K_wZ +Z^TK_wZ) + \frac{\beta}{2} \ Tr(DZ) + \frac{\mu}{2}(-2Tr(E^TZ) + Tr(Z^TZ))
\end{split}
\end{equation*}

On pose donc : $\tilde{F}(Z) = \text{Tr}(- 2K_wZ +Z^TK_wZ) + \frac{\beta}{2} \ Tr(DZ) + \frac{\mu}{2}(-2Tr(E^TZ) + Tr(Z^TZ))$, et on cherche donc à annuler la dérivée partielle en Z de cette fonction.

\begin{equation*}
\begin{split}
\frac{\partial \tilde{F}(Z)}{\partial Z} &= -2K_w^T + K_wZ + K_w^TZ + \frac{\beta}{2} D^T - \mu E^T +\frac{\mu}{2}(IZ + I^TZ)\\
&= -2K_w + 2 K_wZ + \frac{\beta}{2} D - \mu E +\mu Z\\
&= -2K_w + \frac{\beta}{2} D - \mu E +(\mu I + 2 K_w)Z
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
\frac{\partial \tilde{F}(Z)}{\partial Z} = 0 &\Leftrightarrow -2K_w + \frac{\beta}{2} D - \mu E +(\mu I + 2 K_w)Z = 0\\
&\Leftrightarrow  (\mu I + 2 K_w)Z = 2K_w - \frac{\beta}{2} D + \mu E\\
&\Leftrightarrow  Z = (\mu I + 2 K_w)^{-1}(2K_w - \frac{\beta}{2} D + \mu E)
\end{split}
\end{equation*}

\begin{equation}
Z = (\mu I + 2 K_w)^{-1}(2K_w - \frac{\beta}{2} D + \mu E)
\end{equation}

### 3.2.3. Y

$Y$ est le multiplicateur de lagrange associé au problème. Ce paramètre gère le lien entre les matrices $S$ et $Z$ et évolue selon celui-ci. En effet, plus $S$ et $Z$ seront éloignées, plus les valeurs dans $Y$ seront grandes, et inversement.
De plus, lors de la mise à jour de $Y$ un paramètre d'apprentissage, de pénalisation, $\mu$ intervient, permettant d'ajuster le poids de la contrainte d'égalité.

\begin{equation}
Y = Y + \mu(S-Z)
\end{equation}

### 3.2.4. P

Il s'agit de la variable la plus compliquée à optimiser de l'algorithme. On cherche donc la matrice $P$ telle que :
$$
\underset{P}{\mathrm{argmin}}\ \text{Tr}(P^TLP) + \gamma \ || F - PQ||^2_F , \text{ s.c. }P^TP=I
$$

Si on s'occupait que de trouver $\underset{P}{\mathrm{argmin}}\ \text{Tr}(P^TLP), \text{ s.c. }P^TP=I$, on serait dans la démarche du spectral clustering. Pour trouver le $P$ optimal, il nous suffirait de faire la décomposition en valeurs singulières de $L$, puis de prendre les $c$ dernières valeurs propres, et enfin normaliser chaque ligne.

A FINIR

### 3.2.5. Q

Pour être à jour la matrice $Q$, nous devons résoudre le problème
$$
\underset{Q}{\mathrm{argmin}}\ \gamma \ || F - PQ||^2_F, \text{ s.c. } Q^TQ=I
$$
Ceci est un problème de Procruste orthogonal, avec pour matrice cible $F$ et la matrice à transformer $P$.
Afin de le résoudre, il nous suffit de suivre la démarche présentée dans l'article consacré (disponible ici : [**lien**](https://link.springer.com/article/10.1007/BF02289451)).

Tout d'abord, il nous faut faire la décomposition en valeurs singulières de la matrice $M =F Y^T$, nous donnant $M = U\Sigma V^T$. Ensuite, on obtient $Q$ par simple produit des parties gauche et droite de la décomposition :

\begin{equation}
Q = U V^T
\end{equation}

### 3.2.6. F

Notre objectif est de trouver la matrice $F$ telle qu'elle corresponde à : 
$$
\underset{F}{\mathrm{argmin}} \ \gamma \ || F - PQ||^2_F, \text{ s.c. }  F \in \text{Idx}
$$

\begin{equation*}
\begin{split}
\underset{F}{\mathrm{argmin}} \ \gamma \ || F - PQ||^2_F &= \underset{F}{\mathrm{argmin}} \  ||F||_F^2 - 2 <F,PQ>_F + ||PQ||^2_F\\
&= \underset{F}{\mathrm{argmin}} \  \underbrace{\text{Tr}(F^TF)}_{= \ n} - 2 \text{Tr}(F^TPQ) \\
&= \underset{F}{\mathrm{argmax}} \ \text{Tr}(F^TPQ) \\
\end{split}
\end{equation*}

Pour maximiser cette quantité, $\text{s.c. }  F \in \text{Idx}$, on constate que l'on doit construire F telle que :

\begin{equation}
F_{ij}= \left\{
    \begin{array}{ll}
        1 & \mbox{si } j=\underset{k}{\mathrm{argmax}}(PQ)_{ik} \\
        0 & \mbox{sinon.}\\
    \end{array}
\right.
\end{equation}

### 3.2.7. w

On cherche
$$
\underset{w}{\mathrm{argmin}} \sum_{i=1}^r w_ih_i , \text{ s.c. } \sum_{i=1}^r \sqrt{w_i}=1, w_i\geq 0
$$
avec $h_i = \text{Tr}(K^i - 2K^iZ +Z^TK^iZ)$, où $K^i$ représente le i-ème kernel.

On peut écrire le lagrangien du problème :

$$
J(w) = w^Th + \xi (1- \sum_{i=1}^r \sqrt{w_i})
$$

\begin{equation*}
\begin{split}
\frac{\partial J(w)}{\partial w_i} = 0 &\Leftrightarrow h_i - \frac{\xi}{2\sqrt{w_i}} = 0\\
&\Leftrightarrow  h_i = \frac{\xi}{2\sqrt{w_i}}\\
&\Leftrightarrow  \sqrt{w_i} = \frac{\xi}{2h_i}
\end{split}
\end{equation*}

Par les conditions de Karush-Kuhn-Tucker (KKT), on veut que $\xi (1- \sum_{i=1}^r \sqrt{w_i}) = 0$, on suppose aussi que $\gamma$ est différent de 0. On arrive donc à l'équation :

\begin{equation*}
\begin{split}
\sum_{j=1}^r \sqrt{w_j} &= 1\\
\sum_{j=1}^r \frac{\xi}{2h_j} &= 1\\
\frac{1}{\xi} &= \sum_{j=1}^r \frac{1}{2h_j}\\
\xi  &= \Big(\sum_{j=1}^r \frac{1}{2h_j} \Big)^{-1}
\end{split}
\end{equation*}

On revient à $w_i$

\begin{equation*}
\begin{split}
\sqrt{w_i} &= \frac{\xi}{2h_i}\\
\sqrt{w_i} &= \frac{\Big(\sum_{j=1}^r \frac{1}{2h_j} \Big)^{-1}}{2h_i}\\
\sqrt{w_i} &= \Big({h_i}\sum_{j=1}^r \frac{1}{h_j}  \Big)^{-1}\\
w_i &= \Big({h_i}\sum_{j=1}^r \frac{1}{h_j}  \Big)^{-2}
\end{split}
\end{equation*}


\begin{equation}
w_i = \Big({h_i}\sum_{j=1}^r \frac{1}{h_j}  \Big)^{-2}, \ \forall i \in \{1,\ldots,r\}
\end{equation}

## 3.3. Algorithme
\begin{algorithm}
\caption{Spectral Clustering with Multiple Kernels}
\begin{algorithmic} 
\REQUIRE Nombre de clusters $c$, ensemble de kernels $\{K^i\}_{i=1}^{r}$, paramètres $\alpha,\beta,\gamma,\mu>0$.
\STATE Initialisation : ...
\WHILE{Le critère d'arrêt n'est pas satisfait}
\STATE Calculer $K_w$
\STATE Mise à jour de $S$ par (1).
\STATE $S_{ii}=0,\ \forall i \in \{1,\ldots,n\}$ et $S_{ij}=\max(S_{ij},0)$.
\STATE Mise à jour de $Z$ par (2).
\STATE $Z_{ii}=0,\ \forall i \in \{1,\ldots,n\}$ et $Z_{ij}=\max(Z_{ij},0)$, puis $Z = \frac{Z + Z^T}{2}$.
\STATE Mise à jour de $Y$ par (3).
\STATE Mise à jour de $P$
\STATE Mise à jour de $Q$ par (4).
\STATE Mise à jour de $F$ par (5).
\STATE Mise à jour de $w$ par (6).
\ENDWHILE
\end{algorithmic}
\end{algorithm}


# 4. Exemple(s)

## 4.1. 3 Gaussiennes 

## 4.2. Autre exemple plus compliqué (1 noyau puis mélange)


# Autre version :

Remplacer le fait de trouver Q et F, par un K-means sur P.

# 5. Discussion

- Temps de calcul
- Optimisation hyper-paramètres
- Souci de stabilité

# Conclusion


